<h2 id="september-2021">11 September, 2021</h2>
<h3 id="introduction">Introduction</h3>
<p>This Tuesday, something <em>really</em> bad happened on floor at Computer Science House. For a brief moment, our entire network infrastructure… flickered. We aren’t quite sure why it happened, or even what <em>exactly</em> happened, but the effect was that, for just long enough, seven of our Proxmox 6.3 nodes (that’s about half of our virtualized infrastructure) lost their network connection, causing them to fence, and start dumping virtual machines.</p>
<p>Lots of stuff crashed. We lost a chunk of our IPA cluster, we lost our main webserver, we lost an untold number of user VMs that may or may not have been restarted yet, and with all of that compute infrastructure, most of our house services went down: Gitlab, conditional, STARRS (our DNS frontend), and I don’t even remember how many others. The cherry on top was when Datadog started logging downtime. Damb.</p>
<p>In addition to everything else that ker-ploded, one of our OKD 4 workers went down. In hindsight, it’s a miracle we didn’t lose more. I’m writing this article to <em>finally</em> document some of the common pitfalls that people (people being myself) experience when trying to bring this back up.</p>

<figure>
<img src="posts/images/datadog_downtime.png" alt="red bar moment" style="width:250px" />
</figure>
<p>So that day, I worked for 10 hours, then, sometime around 6:00pm, this happened. I checked my phone when I got home, got <em>back in</em> the car, and drove to floor to start putting out fires. We were there for 7 hours.</p>
<p>All told, it wasn’t actually <em>that</em> bad. Our systems were pretty resilient and HA was configed (more on that later). After that night, we had managed to bring IPA back up (sans Radius, need to fix that still), and just yesterday, we fixed Gitlab. So, stuff is now on the upswing <strike>(except for our UPS, that’s catching on fire right now but it’s fine I’ll blog about it later)</strike></p>
<h3 id="the-issue">The issue</h3>
<p>So, okd4-worker03-nrh.csh.rit.edu went down. It stayed down. In the past when this has happened, I’ve had limited success bringing workers back up. Sometimes, you’ll boot them up, and they’ll just immediately start trying to boot to the CoreOS installer, or they won’t boot at all, or they’ll boot loop, and here’s why:</p>
<h1 id="were-running-our-cluster-on-proxmox." style="text-align: center">We’re running our cluster on Proxmox.</h1>
<p>So two things are usually wrong when a node won’t come back up:</p><p> <strong>1:</strong> The “Boot Order” option in the “Options” tab is misconfigured. It may, for some reason, have decided to set the install disk as the primary boot option, or it may have disabled the hard drive option. Either way, you need to make sure that the hard drive is the primary boot option.</p>
<figure>
    <img src="posts/images/boot_order.png" alt="This field is the bane of my existance" style="width:70%" />
</figure>
<p><strong>2:</strong> HA has gotten <em>very</em> confused. I find that the best way to ensure success when bringing an OKD 4 worker back up is to set HA to ‘ignored.’ In this particular case, it was set to ‘stopped’ for some odd reason. That caused this weird issue where, no matter what setting I changed, and no matter how I went about turning the node off and on again, the damn thing just <em>wouldn’t</em> boot correctly. Either it would boot to the installer, or it would boot loop, or it would just <em>ignore</em> me. At one point, I pressed ‘Stop’ three times (and all three times Proxmox logged the Status as OK on those commands) in order to get it to actually stop. Once, I removed literally every storage medium and it still insisted on booting to the installer somehow. <strong>So, TL;DR: SET HA TO ‘IGNORED’ BEFORE MESSING WITH IT.</strong></p>
<figure>
<img src="posts/images/haaaaa.png" alt="So is this one" style="width:200px" />
</figure>
<h3 id="so-the-vm-is-behaving-properly-now-what">So the VM is behaving properly, now what?</h3>
<p>Well, in a panic that night, and either forgetting about or not yet discovering any of the above information, I made the mistake of deciding to just delete the node from the cluster and re-ignite it. <strong>It is important to note that this is almost never necessary. Workers are stupid drones. They just do whatever they are told.</strong> They don't store any cluster critical information, and if one of them blinks out (like ours did), the cluster can easily recover and keep going. Then, you can just turn the node back on, and the cluster picks up where it left off.</p>
<p>So I did a dumb thing: <code>oc delete node okd4-worker03-nrh</code>. I then proceeded to attempt to re-ignite the node. This proved problematic, though, because, for context, we’ve been having issues with the <code>oc</code> console on our services VM. So, that very same Tuesday night, the console had broken again, giving us various “unauthorized” and “cert signed by unknown authority” errors. Very annoying. I’ll be blogging about how we fixed that later (and I’ll have to do a deep dive into that issue at some point because our solution is more of a workaround and less of a fix). To fix the console, basically, we re-create the Kubeconfig. As you might imagine, this changes certificates, which are an integral part of the ignition process. This means that our current ignition setup simply… doesn’t work. According to most RedHat documentation I can find, you’re supposed to hold onto your ignition files. The problem with this is, as I mentioned, our certs got all munged by our console malarkey.</p>
<p>So I go to look for resources that may help me figure out how to create <em>new</em> ignition configs. There is, of course, the option of using <code>openshift-install</code> to regenerate the whole thing using an install manifest. This, naturally, didn’t work.</p>
<p>There’s three main parts of this whole setup that could be wrong:</p>
<ul>
<li>The version of Fedora CoreOS</li>
<li>The version of <code>openshift-install</code></li>
<li>The Certificate Authority we’re passing to the installer</li>
</ul>
<p>Last time I did this, I used a pretty old version; Maybe I had to futz with the Certificate Authority a bit, but it worked. This time, though, no such luck. When I started mixing and matching from this category, I would get:</p>
<ul>
<li>“Certificate signed by unknown authority” errors on the worker ignition console</li>
<li>An actual ignition that spun forever and never gave me a <code>csr</code></li>
<li>A bizarre error: <code>Failed to set up standard input: Inappropriate ioctl for device</code></li>
</ul>
<p>By futz with the Certificate Authority, I mean follow Method #2 on <a href="https://www.linkedin.com/pulse/how-add-new-worker-node-existing-openshift-4-cluster-ibm-miranda/">this blog post.</a> It worked once before, I think…</p>
<h3>The Solution</h3>
<p>So this went on and on for the next few days until last night, when I had the realization that we take <em>really</em> good backups, and maybe, just maybe, if I restored the VM to a copy from Monday night, fixed all the HA nonsense and boot order malarkey, it would request to re-join the cluster on its own.</p>
<figure>
    <img src="posts/images/csr.png" alt="CSRs! FINALLY!" style="width: 100%" />
</figure>
<p>And that’s exactly what happened.</p>
<p>You see, workers don’t really store any critical cluster information. Only what they need to join the cluster themself. So, you can do whatever you want to them, and they’ll come back (to a reasonable extent, of course).</p>
<p>So, if this ever happens to you, don’t panic. Check your infrastructure first, and only use reignition as a <em>last resort.</em></p>
<p>(And take some friggin’ backups)</p>
<p>(Now it’s time for me to figure out why I can’t ignite new workers. God, I need to do some OKD housekeeping. Our services homedir looks like it was hit by a tornado)</p>
